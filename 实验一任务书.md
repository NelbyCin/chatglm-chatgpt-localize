实验一任务

[实验内容]

1. 设计一个Prompt，完成选题中的一个任务。

2. 需要同时部署本地大模型和使用远程大模型。

3. 需要提供后续的相关应用代码。


[完成人数]
1. 这次作业鼓励单人完成。

2. 本次的部署和之后的作业/期末紧密相关，请认真完成。

3. 单人不限制本地模型的部署类型，3人完成最多和另一个3人的模型一样，6人完成必须独立一种大模型。

4. 无论模型是否和别的组一样，均需要独立完成部署

[选题]

知识挖掘类Prompt：

KE1. 给定一段文本，抽取其中常识知识三元组，每一个三元组的组织形式为（头实体，关系，尾实体）。

KE2. 给定一段文本，抽取其中存在的命名实体（人物、地点、机构等），分每一个类别存储。

KE3. 给定一段文本，以及指定的实体（任五、地点、机构等），抽取与该实体相关的Key-Value知识。


Question-Answer 类Prompt：

QA1. 给定一段背景材料和一个问题和四个答案（A、B、C、D）， 要求模型输出对应的答案以及理由。

QA2. 给定一段背景材料和一个问题， 要求模型写出对应的答案以及理由。

QA3. 给定一段背景材料，要求模型从中发现问题以及答案（即生成问题、答案对）。 

QA4. 给定一段背景材料，以及指定的人物/地点/时间等，要求模型根据给定的信息生成问题和答案。

QA5. 给定一段背景材料，题目，以及正确答案（文字），以及一个学生答案。 设计一个评分Prompt让模型对这个答案进行打分，评分需要是从多个有不同具体含义的指标进行，打分需要在1-10分之间。


[评分要求]

基本要求（每一个⭐️=10分）：

1. [Remote LLM 测试 ⭐️⭐️]      自己准备不少于3个例子，在远程大模型上测试。
2. [Local LLM 测试效果  ⭐️⭐️]   自己准备不少于3个例子（保持同上），在本地大模型上测试。
3. [Local LLM 部署情况 ⭐️]      检查是否已经在本地部署了大模型
4. [Local LLM 应用开发 ⭐⭐️⭐️]    是否已经将所选定的大模型、任务封装成了一个可以直接调用的代码（实现批量输入、批量输出、错误异常管理等，需要自己准备大量测试数据）
5. [文档⭐️⭐️]                  本次作业每个小组都必须以Github/Gitlab项目的方式提交，其中的Readme.md 为本次实验报告的文档。


额外政策：
1. 第二周开始可以检查测试效果和部署效果（前三项），检查周数和分数上限的关系为：第二周-100分，第三周-95分，第四周85分，第五周80分，第六周75分，之后只记60分。
2. 后两项必须在第前三项检查完后再做检查，原则上为检查完两次课内完成（可能会要求返修），之后每延迟一周上限扣减10分，最低上限75。
3. 原则上要获得高分，需要同时提供中文和英文版本的Prompt，给分优先度为： 中文+英文 > 英文 > 中文。


因为大家普遍在第一次检查时存在诸多不合格，请注意第二次检查需要检查如下内容：
1. 无论是自己部署的LLM还是远端LLM API，都需要以HTTP的方式提供独立服务。
2. 文档需要提供从Linux Ubuntu 原始环境部署本地模型的方法，以及对应任务的测试。
3. 应用两种形式（可选或者都选择）：
       a)  提供一个网页实现对应任务（指定Prompt，指定LLM，输出结果）。不得使用任何自带的前端，除非同时做了b。
       b)  提供一个命令行的方式，通过argparser 配置参数，批量读取批量输出，需要考虑断点续传、异常处理等。
4. 无论选择任何形式，所展现的结果一定1）不需要自己写Prompt，输入中不能包含Prompt。2）输出已经要抽取了数据，不得直接使用大模型返回的结果。




python chatglm_cpp/convert.py -i ZhipuAI/chatglm3-6b -t q4_0 -o chatglm-ggml.bin
./build/bin/Release/main -m chatglm3-ggml.bin -i



